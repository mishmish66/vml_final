{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax_verify\n",
    "import numpy as np\n",
    "import optax\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from einops import reduce\n",
    "from flax import nnx\n",
    "from orbax import checkpoint as ocp\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "\n",
    "from vml_final.data import CSVDataset, CSVDatasetEpochLoader\n",
    "from vml_final.model import TemporalConvolutionalNetwork\n",
    "from vml_final.training import do_eval_epoch, do_train_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_root = Path(\"../../\")\n",
    "\n",
    "set_paths = [\n",
    "    project_root / \"csv_dataset\" / f\"AB{str(i).zfill(2)}\" for i in range(6, 25)\n",
    "]\n",
    "\n",
    "stack_size = 200\n",
    "train_frac = 0.9\n",
    "\n",
    "sensors = [\"imu\", \"gon\", \"emg\"]\n",
    "# sensors = [\"emg\"]\n",
    "\n",
    "dset_name = f\"6-25_{'-'.join(sensors)}_{stack_size}\"\n",
    "ckpt_path = (project_root / \"processed_sets\" / dset_name).resolve()\n",
    "\n",
    "\n",
    "build_dset = not ckpt_path.exists() # True\n",
    "\n",
    "\n",
    "rngs = nnx.Rngs(0)\n",
    "\n",
    "with ocp.StandardCheckpointer() as ckptr:\n",
    "\n",
    "    if build_dset:\n",
    "        dset = CSVDataset.build(\n",
    "            rngs(),\n",
    "            set_paths,\n",
    "            stack_size=stack_size,\n",
    "            sensors_to_use=sensors,\n",
    "            train_frac=train_frac,\n",
    "        )\n",
    "        ckptr.save(ckpt_path, dset)\n",
    "    else:\n",
    "        restored_list = ckptr.restore(ckpt_path)\n",
    "        dummy = CSVDataset(\n",
    "            *restored_list\n",
    "        )\n",
    "        dset = ckptr.restore(ckpt_path, dummy)\n",
    "        stack_size = dset.stack_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset.x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = CSVDatasetEpochLoader(dset, 32_768)\n",
    "eval_loader = CSVDatasetEpochLoader(dset, 32_768, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir ../../logs --port 6006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TemporalConvolutionalNetwork(\n",
    "    input_channels=dset.x.shape[-1],\n",
    "    conv_hidden_dims=[4, 4, 4, 8],\n",
    "    # mlp_hidden_dims=[128, 128],\n",
    "    kernel_size=7,\n",
    "    stride=6,\n",
    "    dropout=0.2,\n",
    "    rngs=rngs,\n",
    ")\n",
    "\n",
    "optim = nnx.Optimizer(model, optax.adam(2.5e-3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter(project_root / \"logs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jax.config.update(\"jax_debug_nans\", True)\n",
    "jax.config.jax_debug_nans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 128\n",
    "\n",
    "# pbar.close()\n",
    "pbar = tqdm(total=num_epochs)\n",
    "\n",
    "\n",
    "def logging_callback(train_loss, validation_loss, step):\n",
    "    pbar.update()\n",
    "    pbar.set_postfix(\n",
    "        {\"Train Loss\": train_loss.item(), \"Val Loss\": validation_loss.item()}\n",
    "    )\n",
    "    writer.add_scalars(\n",
    "        \"loss\",\n",
    "        {\"train\": train_loss.item(), \"validation\": validation_loss.item()},\n",
    "        global_step=step,\n",
    "    )\n",
    "\n",
    "\n",
    "optim_graphdef, optim_state = nnx.split(optim)\n",
    "\n",
    "\n",
    "def scanf(optim_state, key):\n",
    "    optim = nnx.merge(optim_graphdef, optim_state)\n",
    "    rngs = nnx.Rngs(key)\n",
    "    train_loss = do_train_epoch(optim, train_loader, rngs=rngs)\n",
    "    validation_loss = do_eval_epoch(optim.model, eval_loader, rngs=rngs)\n",
    "\n",
    "    current_step_index = optim.step.value\n",
    "    jax.debug.callback(\n",
    "        logging_callback, train_loss, validation_loss, current_step_index\n",
    "    )\n",
    "\n",
    "    return nnx.state(optim), None\n",
    "\n",
    "\n",
    "optim_state, _ = jax.lax.scan(scanf, optim_state, jax.random.split(rngs(), num_epochs))\n",
    "nnx.update(optim, optim_state)\n",
    "\n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_x, batch_y = dset.get_batch(rngs(), 128, train=False)\n",
    "\n",
    "np.stack([model(batch_x), batch_y], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_eval_epoch(optim.model, eval_loader, rngs=rngs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnx.display(optim.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_max = reduce(dset.x, \"e c -> c\", \"max\")\n",
    "channel_min = reduce(dset.x, \"e c -> c\", \"min\")\n",
    "\n",
    "upper = np.array([channel_max] * stack_size)\n",
    "lower = np.array([channel_min] * stack_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_graphdef, model_state = nnx.split(model)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "@jax.jit\n",
    "def pure_call(x):\n",
    "    model = nnx.merge(model_graphdef, model_state)\n",
    "    return model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_bound = jax_verify.backward_crown_bound_propagation(\n",
    "    model,\n",
    "    jax_verify.IntervalBound(lower, upper),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_bound.lower, output_bound.upper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now let's verify it for particular common speeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_bound(center, to_add):\n",
    "    return jax_verify.IntervalBound(center - to_add, center + to_add)\n",
    "\n",
    "\n",
    "def crown_verify(centers, stdevs, factor=0.01):\n",
    "    def crown(center):\n",
    "        output_bound = jax_verify.backward_crown_bound_propagation(\n",
    "            optim.model, make_bound(center, speed_to_add)\n",
    "        )\n",
    "        return output_bound.lower, output_bound.upper\n",
    "\n",
    "    return jax.vmap(crown)(centers)\n",
    "\n",
    "\n",
    "def noise_verify(centers, stdevs, factor=0.1, samples_per_center=16):\n",
    "\n",
    "    def verify(center, key):\n",
    "        normal_samples = jax.random.normal(key, (samples_per_center, *center.shape))\n",
    "        perturbed_inputs = normal_samples * stdevs * factor + center\n",
    "        outputs = optim.model(perturbed_inputs)\n",
    "        return jnp.min(outputs, axis=0), jnp.max(outputs, axis=0)\n",
    "\n",
    "    center_lower, center_upper = jax.vmap(verify)(centers, jax.random.split(rngs(), len(centers)))\n",
    "\n",
    "    return jnp.min(center_lower, axis=0), jnp.max(center_upper, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniques, counts = np.unique_counts(dset.y)\n",
    "common_speeds = uniques[counts > 2048]\n",
    "\n",
    "range_size = []\n",
    "\n",
    "for common_speed in common_speeds:\n",
    "    speed_idxs = dset.validation_idxs[dset.y[dset.validation_idxs] == common_speed]\n",
    "    speed_x, speed_y = dset[speed_idxs]\n",
    "\n",
    "    speed_channel_x_stds = jnp.std(speed_x, axis=0)\n",
    "    std_factor = 0.1\n",
    "    speed_to_add = speed_channel_x_stds * std_factor\n",
    "\n",
    "    # speed_bound = make_full_bound(speed_x, speed_to_add)\n",
    "\n",
    "    def crown(x):\n",
    "        output_bound = jax_verify.backward_crown_bound_propagation(\n",
    "            optim.model, make_bound(x, speed_to_add)\n",
    "        )\n",
    "        return output_bound.lower, output_bound.upper\n",
    "\n",
    "    if len(speed_x) > 8:\n",
    "        speed_x = jax.random.choice(rngs(), speed_x, (8,), replace=False)\n",
    "    speed_output_lowers, speed_output_uppers = jax.vmap(crown)(speed_x)\n",
    "\n",
    "    speed_output_lower = jnp.min(speed_output_lowers)\n",
    "    speed_output_upper = jnp.max(speed_output_uppers)\n",
    "    \n",
    "    range_size.append(speed_output_upper - speed_output_lower)\n",
    "    \n",
    "    print(\n",
    "        f\"For speed: {common_speed} bounds are: {speed_output_lower} to {speed_output_upper}\"\n",
    "    )\n",
    "    \n",
    "print(f\"Mean range size: {np.mean(range_size)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniques, counts = np.unique_counts(dset.y)\n",
    "common_speeds = uniques[counts > 200]\n",
    "\n",
    "range_size = []\n",
    "\n",
    "for common_speed in common_speeds:\n",
    "    speed_idxs = dset.validation_idxs[dset.y[dset.validation_idxs] == common_speed]\n",
    "    speed_x, speed_y = dset[speed_idxs]\n",
    "\n",
    "    speed_channel_x_stds = jnp.std(speed_x, axis=0)\n",
    "    std_factor = 0.01\n",
    "    speed_to_add = speed_channel_x_stds * std_factor\n",
    "\n",
    "    if len(speed_x) > 256:\n",
    "        speed_x = jax.random.choice(rngs(), speed_x, (256,), replace=False)\n",
    "\n",
    "    speed_output_lowers, speed_output_uppers = noise_verify(\n",
    "        speed_x, stdevs=speed_channel_x_stds, factor=std_factor\n",
    "    )\n",
    "\n",
    "    speed_output_lower = jnp.min(speed_output_lowers)\n",
    "    speed_output_upper = jnp.max(speed_output_uppers)\n",
    "\n",
    "    range_size.append(speed_output_upper - speed_output_lower)\n",
    "\n",
    "    print(\n",
    "        f\"For speed: {common_speed} bounds are: {speed_output_lower} to {speed_output_upper}\"\n",
    "    )\n",
    "\n",
    "print(f\"Mean range size: {np.mean(range_size)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now if we are using all sensors lets try perturbing each one randomly separately\n",
    "\n",
    "sensor_dims = {\n",
    "    \"imu\": 25,\n",
    "    \"gon\": 6,\n",
    "    \"emg\": 12,\n",
    "    # \"imu\": 25, \"gon\": 6, \"emg\": 12\n",
    "}\n",
    "\n",
    "sensor_ranges = {}\n",
    "running = 0\n",
    "for sensor in sensors:\n",
    "    sensor_dim = sensor_dims[sensor]\n",
    "    sensor_ranges[sensor] = slice(running, running + sensor_dim)\n",
    "    running += sensor_dim\n",
    "\n",
    "uniques, counts = np.unique_counts(dset.y)\n",
    "common_speeds = uniques[counts > 200]\n",
    "\n",
    "std_factor = 0.025\n",
    "\n",
    "result_dict = {\n",
    "    \"Speed\": [],\n",
    "    **{f\"{sensor} Perturbation\": [] for sensor in sensors}\n",
    "}\n",
    "\n",
    "for common_speed in common_speeds:\n",
    "    speed_idxs = dset.validation_idxs[dset.y[dset.validation_idxs] == common_speed]\n",
    "    speed_x, speed_y = dset[speed_idxs]\n",
    "\n",
    "    speed_channel_x_stds = jnp.std(speed_x, axis=0)\n",
    "    speed_to_add = speed_channel_x_stds * std_factor\n",
    "\n",
    "    if len(speed_x) > 256:\n",
    "        speed_x = jax.random.choice(rngs(), speed_x, (32,), replace=False)\n",
    "\n",
    "    for sensor in sensors:\n",
    "        sensor_channel_x_std = (\n",
    "            jnp.zeros_like(speed_channel_x_stds)\n",
    "            .at[..., sensor_ranges[sensor]]\n",
    "            .set(speed_channel_x_stds[..., sensor_ranges[sensor]])\n",
    "        )\n",
    "\n",
    "        def verify(center, key):\n",
    "            normal_samples = jax.random.normal(key, (64, *center.shape))\n",
    "            perturbed_inputs = (\n",
    "                normal_samples * sensor_channel_x_std * std_factor + center\n",
    "            )\n",
    "            outputs = optim.model(perturbed_inputs)\n",
    "            return jnp.min(outputs, axis=0), jnp.max(outputs, axis=0)\n",
    "\n",
    "        speed_output_lowers, speed_output_uppers = jax.vmap(verify)(\n",
    "            speed_x, jax.random.split(rngs(), len(speed_x))\n",
    "        )\n",
    "\n",
    "        # speed_output_lowers, speed_output_uppers = noise_verify(\n",
    "        #     speed_x, stdevs=sensor_channel_x_std, factor=std_factor\n",
    "        # )\n",
    "\n",
    "        ranges = speed_output_uppers - speed_output_lowers\n",
    "\n",
    "        for sample_range in np.array(ranges):\n",
    "            result_dict[f\"{sensor} Perturbation\"].append(sample_range.item())\n",
    "\n",
    "    # Extend speeds to max len\n",
    "    max_len = max(*[len(val) for val in result_dict.values()])\n",
    "    missing_len = max_len - len(result_dict[\"Speed\"])\n",
    "    result_dict[\"Speed\"].extend([common_speed.item()] * missing_len)\n",
    "\n",
    "    print(f\"Did speed {common_speed}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(\n",
    "    result_dict\n",
    ")\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df, hue=\"Speed\"), #kind=\"scatter\", #vars=[\"imu Perturbation\", \"gon Perturbation\", \"emg Perturbation\"])\n",
    "# output_perturbation[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "treadmill",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
